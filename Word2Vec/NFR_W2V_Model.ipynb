{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of requirements: 625\n",
      "   ProjectID                                    RequirementText class\n",
      "0          1  'The system shall refresh the display every 60...    PE\n",
      "1          1  'The application shall match the color of the ...    LF\n",
      "2          1  ' If projected  the data must be readable.  On...    US\n",
      "3          1  ' The product shall be available during normal...     A\n",
      "4          1  ' If projected  the data must be understandabl...    US\n",
      "method split_sentences called 1 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-19 00:36:39,035 : INFO : collecting all words and their counts\n",
      "2017-09-19 00:36:39,038 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-09-19 00:36:39,044 : INFO : collected 1624 word types from a corpus of 12152 raw words and 771 sentences\n",
      "2017-09-19 00:36:39,048 : INFO : Loading a fresh vocabulary\n",
      "2017-09-19 00:36:39,054 : INFO : min_count=10 retains 191 unique words (11% of original 1624, drops 1433)\n",
      "2017-09-19 00:36:39,059 : INFO : min_count=10 leaves 8779 word corpus (72% of original 12152, drops 3373)\n",
      "2017-09-19 00:36:39,060 : INFO : deleting the raw counts dictionary of 1624 items\n",
      "2017-09-19 00:36:39,063 : INFO : sample=0.001 downsamples 73 most-common words\n",
      "2017-09-19 00:36:39,064 : INFO : downsampling leaves estimated 4138 word corpus (47.1% of prior 8779)\n",
      "2017-09-19 00:36:39,064 : INFO : estimated required memory for 191 words and 300 dimensions: 553900 bytes\n",
      "2017-09-19 00:36:39,066 : INFO : resetting layer weights\n",
      "2017-09-19 00:36:39,073 : INFO : training model with 4 workers on 191 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-09-19 00:36:39,133 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-09-19 00:36:39,143 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-09-19 00:36:39,153 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-09-19 00:36:39,156 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-09-19 00:36:39,159 : INFO : training on 60760 raw words (20715 effective words) took 0.1s, 288112 effective words/s\n",
      "2017-09-19 00:36:39,160 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-09-19 00:36:39,161 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-09-19 00:36:39,165 : INFO : saving Word2Vec object under models_nfr/300features_10minwords_10context.model, separately None\n",
      "2017-09-19 00:36:39,167 : INFO : not storing attribute syn0norm\n",
      "2017-09-19 00:36:39,167 : INFO : not storing attribute cum_table\n",
      "2017-09-19 00:36:39,176 : INFO : saved models_nfr/300features_10minwords_10context.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 130 ms, sys: 10 ms, total: 140 ms\n",
      "Wall time: 144 ms\n",
      "625 reviews -> 771 sentences\n",
      "Training model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nSome examples of how the model works\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import nltk.data\n",
    "#nltk.download()\n",
    "#nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "#!pip install gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "def load_dataset(name, nrows=None):\n",
    "    datasets = {\n",
    "        'unlabeled_train': 'nfr.csv',\n",
    "        'labeled_train': 'labeledTrainData.tsv',\n",
    "        'test': 'testData.tsv'\n",
    "    }\n",
    "    if name not in datasets:\n",
    "        raise ValueError(name)\n",
    "    data_file = os.path.join('data_nfr', datasets[name])\n",
    "    df = pd.read_csv(data_file, sep=',', escapechar='\\\\', nrows=nrows,encoding = \"ISO-8859-1\")\n",
    "    print('Number of requirements: {}'.format(len(df)))\n",
    "    return df\n",
    "\n",
    "'''\n",
    "First load in the unlabeled dataset.\n",
    "This will be used for training the word vectors.\n",
    "'''\n",
    "df = load_dataset('unlabeled_train')\n",
    "print(df.head())\n",
    "\n",
    "'''\n",
    "Next, clean the text, similar to part 1 and divide into sentences\n",
    "However, this time do not remove stopwords. Sentences are split using the nltk punkt tokenizer. \n",
    "The result is a list of sentences obtained from all the requirements combined, and each sentence is \n",
    "a list of cleaned words (still including stopwords).\n",
    "'''\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "eng_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text, remove_stopwords=False):\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    words = text.lower().split()\n",
    "    if remove_stopwords:\n",
    "        words = [w for w in words if w not in eng_stopwords]\n",
    "    return words\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def print_call_counts(f):\n",
    "    n = 0\n",
    "    def wrapped(*args, **kwargs):\n",
    "        nonlocal n\n",
    "        n += 1\n",
    "        if n % 1000 == 1:\n",
    "            print('method {} called {} times'.format(f.__name__, n))\n",
    "        return f(*args, **kwargs)\n",
    "    return wrapped\n",
    "\n",
    "@print_call_counts\n",
    "def split_sentences(req):\n",
    "    raw_sentences = tokenizer.tokenize(req.strip())\n",
    "    sentences = [clean_text(s) for s in raw_sentences if s]\n",
    "    return sentences\n",
    "\n",
    "%time sentences = sum(df.RequirementText.apply(split_sentences), [])\n",
    "print('{} reviews -> {} sentences'.format(len(df), len(sentences)))\n",
    "\n",
    "#get_ipython().magic('time sentences = sum(df.review.apply(split_sentences), [])')\n",
    "#print('{} reviews -> {} sentences'.format(len(df), len(sentences)))\n",
    "\n",
    "\n",
    "'''\n",
    "Here we will train the word vector model\n",
    "Default logging setup and parameters taken from the tutorial.\n",
    "'''\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality\n",
    "min_word_count = 10   # Minimum word count\n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size\n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "model_name = '{}features_{}minwords_{}context.model'.format(num_features, min_word_count, context)\n",
    "\n",
    "print('Training model...')\n",
    "model = Word2Vec(sentences, workers=num_workers,\n",
    "\tsize=num_features, min_count = min_word_count,\n",
    "\twindow = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model.save(os.path.join('models_nfr', model_name))\n",
    "\n",
    "'''\n",
    "Some examples of how the model works\n",
    "'''\n",
    "\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "\n",
    "#print(model.doesnt_match(\"pool load secure heartbeat\".split()))\n",
    "#print(model.doesnt_match('secure session authorize authenticate'.split()))\n",
    "\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "\n",
    "#model.most_similar(\"pooling\")\n",
    "\n",
    "\n",
    "# In[22]:\n",
    "\n",
    "\n",
    "#model.most_similar(\"authenticate\")\n",
    "\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "\n",
    "#model.most_similar(\"load\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
